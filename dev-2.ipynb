{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from collections import ChainMap\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textanalyzer import MecabTokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv('/root/beauty/tok_glpk.csv', header=None).sample(10000, random_state=42)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MecabTokenization(custom_dir='/root/custom_dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_docs=[]\n",
    "for i, doc in enumerate(docs):\n",
    "    proc_docs.append(tokenizer.mecab.morphs(doc))\n",
    "    \n",
    "result = list(map(lambda doc:Counter(doc), proc_docs))\n",
    "\n",
    "def token2count(tokens):\n",
    "    \"\"\"\n",
    "    list of Tokens -> counts\n",
    "    \"\"\"\n",
    "    pos = {'KEYPHRASE', 'NNG', 'NNP', 'VV', 'VA', 'XR', 'SL'} # 이건 Init으로 빼자.\n",
    "    keywords = []\n",
    "    for tok in tokens:\n",
    "        if set(tok._pos.split('+')).intersection(pos):\n",
    "            # stemming\n",
    "            if '+' in tok._pos:\n",
    "                for s in tok.expression.split('+'):\n",
    "                    a, b, _= s.split('/')\n",
    "                    if b in pos:\n",
    "                        stem = a\n",
    "                        p = b\n",
    "            else:\n",
    "                stem = tok.text\n",
    "                p = tok._pos\n",
    "\n",
    "            # lemmantization\n",
    "            if p in {'VV', 'VA'}:\n",
    "                stem = f\"{stem}다\"\n",
    "            keywords.append(stem)        \n",
    "    return Counter(keywords)\n",
    "\n",
    "result = list(map(lambda doc:Counter(doc), proc_docs))\n",
    "\n",
    "def document_term_matrix(count_docs):\n",
    "    \"\"\"\n",
    "    * candidate selection은 preprocesseddocumnets에 구현.\n",
    "    * candidate weighting\n",
    "    \"\"\"\n",
    "    # Define idx2token and token2idx for represent the data by matrix\n",
    "    idx2token = sorted(dict(ChainMap(*count_docs)).keys())\n",
    "    token2idx = {tok:i for i, tok in enumerate(idx2token)}\n",
    "    \n",
    "    # Transform list-of-dict to document-term-matrix using sparse matrix\n",
    "    rows = list(chain(*[[doc_idx] * len(doc) for doc_idx, doc in enumerate(count_docs)])) # for (i,j)~DTM row-wise index position\n",
    "    cols, data = list(zip(*chain(*[doc.items() for doc in count_docs]))) # term keywords, data is frequence\n",
    "    cols = [token2idx[c] for c in cols] # transform term keyword to for (i,j)~DTM column-wise index position\n",
    "    dtm = csr_matrix((data, (rows, cols)))\n",
    "    \n",
    "    return dtm, idx2token, token2idx\n",
    "\n",
    "dtm, idx2token, token2idx = document_term_matrix(result)\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# tfidf = TfidfTransformer(norm=False)\n",
    "# dtm = tfidf.fit_transform(dtm)\n",
    "# scores = np.squeeze(np.asarray(dtm.sum(axis=0))) # document-wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2lm(text):\n",
    "    tokens = tokenizer.mecab.morphs(text)\n",
    "    text_len = len(tokens)\n",
    "    d = {}\n",
    "    for token in tokens:\n",
    "        if token not in d:\n",
    "            d[token] = 0\n",
    "        d[token] += 1\n",
    "    return d, text_len\n",
    "\n",
    "def cnt_corpus(docs):\n",
    "    docs_terms = {}\n",
    "    df = {}\n",
    "    total_df = len(docs)\n",
    "    total_doc_len = 0\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_terms, doc_len = text2lm(doc)\n",
    "        docs_terms[i] = doc_terms\n",
    "        for item in doc_terms:\n",
    "            if item not in df:\n",
    "                df[item] = 0\n",
    "            df[item] += 1\n",
    "        total_doc_len += doc_len\n",
    "    avg_doc_len = total_doc_len / total_df\n",
    "    return docs_terms, df, total_df, avg_doc_len\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.linalg import norm\n",
    "\n",
    "class ClassicExtractor():\n",
    "    # classical feature extractor\n",
    "    def __init__(self, query_terms, doc_terms, df, total_df=None, avg_doc_len=None):\n",
    "        \"\"\"\n",
    "        :param query_terms: query term -> tf\n",
    "        :param doc_terms: doc term -> tf\n",
    "        :param df: term -> df dict\n",
    "        :param total_df: a int of total document frequency\n",
    "        :param avg_doc_len: a float of avg document length\n",
    "        \"\"\"\n",
    "        query_tf = [item[1] for item in query_terms.items()]\n",
    "        query_df = []\n",
    "        doc_tf = []\n",
    "        for item in query_terms.items():\n",
    "            if item[0] in df:\n",
    "                query_df.append(df[item[0]])\n",
    "            else:\n",
    "                query_df.append(0)\n",
    "            if item[0] in doc_terms:\n",
    "                doc_tf.append(doc_terms[item[0]])\n",
    "            else:\n",
    "                doc_tf.append(0)\n",
    "        \n",
    "        self.query_tf = np.array(query_tf)\n",
    "        self.query_df = np.array(query_df)\n",
    "        self.doc_tf = np.array(doc_tf)\n",
    "\n",
    "        self.doc_len = sum([item[1] for item in doc_terms.items()])\n",
    "        if total_df is not None:\n",
    "            self.total_df = total_df\n",
    "        if avg_doc_len is not None:\n",
    "            self.avg_doc_len = avg_doc_len\n",
    "\n",
    "        self.k1 = 1.2\n",
    "        self.b = 0.75\n",
    "        self.dir_mu = 2500\n",
    "        self.min_tf = 0.1\n",
    "        self.jm_lambda = 0.4\n",
    "        self.min_score = 1e-10\n",
    "        return\n",
    "\n",
    "    def get_feature(self):\n",
    "        # l_sim_func = ['lm', 'lm_dir', 'lm_jm', 'lm_twoway',\n",
    "        #               'bm25', 'coordinate', 'cosine', 'tf_idf',\n",
    "        #               'bool_and', 'bool_or']\n",
    "        features = {}\n",
    "        features['lm'] = self.lm()\n",
    "        features['lm_dir'] = self.lm_dir()\n",
    "        features['lm_jm'] = self.lm_jm()\n",
    "        features['lm_twoway'] = self.lm_twoway()\n",
    "        features['bm25'] = self.bm25()\n",
    "        features['coordinate'] = self.coordinate()\n",
    "        features['cosine'] = self.cosine()\n",
    "        features['tf_idf'] = self.tf_idf()\n",
    "        features['bool_and'] = self.bool_and()\n",
    "        features['bool_or'] = self.bool_or()\n",
    "        return features\n",
    "\n",
    "    def lm(self):\n",
    "        if self.doc_len == 0:\n",
    "            return np.log(self.min_score)\n",
    "        v_tf = np.maximum(self.doc_tf, self.min_tf)\n",
    "        v_tf /= self.doc_len\n",
    "        v_tf = np.maximum(v_tf, self.min_score)\n",
    "        score = np.log(v_tf).dot(self.query_tf)\n",
    "        return score\n",
    "\n",
    "    def lm_dir(self):\n",
    "        if self.doc_len == 0:\n",
    "            return np.log(self.min_score)\n",
    "        v_q = self.query_tf / np.sum(self.query_tf)\n",
    "        v_mid = (self.doc_tf + self.dir_mu * (self.query_df / self.total_df)) / (self.doc_len + self.dir_mu)\n",
    "        v_mid = np.maximum(v_mid, self.min_score)\n",
    "        score = np.log(v_mid).dot(v_q)\n",
    "        return score\n",
    "\n",
    "    def lm_jm(self):\n",
    "        if self.doc_len == 0:\n",
    "            return np.log(self.min_score)\n",
    "        v_mid = self.doc_tf / self.doc_len * (1 - self.jm_lambda) + self.jm_lambda * self.query_df / self.total_df\n",
    "        v_mid = np.maximum(v_mid, self.min_score)\n",
    "        score = np.log(v_mid).dot(self.query_tf)\n",
    "        return score\n",
    "\n",
    "    def lm_twoway(self):\n",
    "        if self.doc_len == 0:\n",
    "            return np.log(self.min_score)\n",
    "        v_mid = (self.doc_tf + self.dir_mu * (self.query_df / self.total_df)) / (self.doc_len + self.dir_mu)\n",
    "        v_mid = v_mid * (1 - self.jm_lambda) + self.jm_lambda * self.query_df / self.total_df\n",
    "        v_mid = np.maximum(v_mid, self.min_score)\n",
    "        score = np.log(v_mid).dot(self.query_tf)\n",
    "        return score\n",
    "\n",
    "    def bm25(self):\n",
    "        if self.doc_len == 0:\n",
    "            return 0\n",
    "        v_q = self.query_tf / float(np.sum(self.query_tf))\n",
    "        v_tf_part = self.doc_tf * (self.k1 + 1) / (self.doc_tf + self.k1 * (1 - self.b + self.b * self.doc_len / self.avg_doc_len))\n",
    "        v_mid = (self.total_df - self.query_df + 0.5) / (self.query_df + 0.5)\n",
    "        v_mid = np.maximum(v_mid, 1.0)\n",
    "        v_idf_q = np.log(v_mid)\n",
    "        v_idf_q = np.maximum(v_idf_q, 0)\n",
    "        score = v_mid.dot(v_tf_part * v_idf_q)\n",
    "        score = max(score, 1.0)\n",
    "        score = np.log(score)\n",
    "        return score\n",
    "\n",
    "    def cosine(self):\n",
    "        if self.doc_len == 0:\n",
    "            return 0\n",
    "        if sum(self.doc_tf) == 0:\n",
    "            return 0\n",
    "        v_q = self.query_tf / float(np.sum(self.query_tf))\n",
    "        v_d = self.doc_tf / float(self.doc_len)\n",
    "        score = spatial.distance.cosine(v_q, v_d)\n",
    "        if math.isnan(score):\n",
    "            return 0\n",
    "        return score\n",
    "\n",
    "    def coordinate(self):\n",
    "        return sum(self.doc_tf > 0)\n",
    "\n",
    "    def bool_and(self):\n",
    "        if self.coordinate() == len(self.query_tf):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def bool_or(self):\n",
    "        return min(1, self.coordinate())\n",
    "\n",
    "    def tf_idf(self):\n",
    "        if self.doc_len == 0:\n",
    "            return 0\n",
    "        normed_idf = np.log(1 + self.total_df / np.maximum(self.query_df, 1))\n",
    "        normed_tf = self.doc_tf / self.doc_len\n",
    "        return normed_idf.dot(normed_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_terms, df, total_df, avg_doc_len = cnt_corpus(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18885]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token2idx[term] for term in ['헤라']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ce = ClassicExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = {'헤라' : ['헤라'], '랑콤' : ['랑콤'], '베네피트'  : ['베네피트']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBSAnalyzer:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    def _ngram(self):\n",
    "        return\n",
    "    def token2count(self):\n",
    "        return\n",
    "    def document_term_matrix(self):\n",
    "        return\n",
    "    def network(self):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def prevalence(self):\n",
    "        return\n",
    "    def diversity(self):\n",
    "        return\n",
    "    \n",
    "    def connectivity(self):\n",
    "        return\n",
    "    \n",
    "    def sbs(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- open matchfh rntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_terms, df, total_df, avg_doc_len = cnt_corpus(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df -> document frequency\n",
    "total_df -> total document\n",
    "avg_doc_len -> total_doc_len / total_df # term들의 수\n",
    "\n",
    "query_terms -> vector화\n",
    "docs_terms -> dtm으로 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '설화수'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_terms = np.array([0]*dtm.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, freq in Counter(tokenizer.mecab.morphs(doc)).items():\n",
    "    query_terms[token2idx[term]] += freq\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import diags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x19751 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 570380 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer()\n",
    "dtm = tfidf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.squeeze(np.asarray(dtm.sum(axis=0))) # document-wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  162, 18807,     0,  8475, 14392,   116,   138, 12447, 18327,\n",
       "       16736])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-scores)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  162, 14462,  5665,     0, 18409,  3696, 13129,   116, 14357,\n",
       "       14379])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-scores)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm=False)\n",
    "dtm = tfidf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfTransformer(use_idf=False, sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdtm = tf.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01480299, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01621181, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18221219,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.25677745,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.64724310e+04, 1.59689902e+03, 6.75585038e+02, ...,\n",
       "       9.51729319e+00, 4.95871813e+01, 9.51729319e+00])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 19751)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df\n",
    "(dtm > 0).sum(axis=0) # (1, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total_df \n",
    "dtm.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.7427"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avg_doc_len\n",
    "dtm.sum() / dtm.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6409, 9043, 5015, ..., 7151,  466, 4999])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-dtm.dot(query_terms.reshape(-1,1)).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'moisturizing'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arr\n",
    "idx2token[999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tdm = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x19751 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 570380 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyExtractor:\n",
    "    def __init__(self, dtm, idx2token):\n",
    "        self.dtm = dtm # document-term matrix (document, term)\n",
    "        self.idx2tokens = idx2token # term-list\n",
    "        \n",
    "        # document frequency\n",
    "        self.df = (dtm > 0).sum(axis=0) # document frequency (1, term)\n",
    "        self.total_df = dtm.shape[0]\n",
    "        self.avg_doc_len = dtm.sum() / dtm.shape[0]\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.k1 = 1.2\n",
    "        self.b = 0.75\n",
    "        self.dir_mu = 2500\n",
    "        self.min_tf = 0.1\n",
    "        self.jm_lambda = 0.4\n",
    "        self.min_score = 1e-10\n",
    "    \n",
    "    def bm25(self):\n",
    "        v_tf_part = self.dtm * (self.k1 + 1) / (self.dtm + self.k1 * (1 - self.b + self.b * self.dtm.sum(axis=1) / self.avg_doc_len))        \n",
    "        result=[]\n",
    "        for i in range(len(self.idx2tokens)):\n",
    "            v_mid = np.eye(len(self.idx2tokens), 1 ,i) * (self.total_df - 1 + 0.5)/ (1 + 0.5)\n",
    "            v_idf_q = np.eye(1,len(self.idx2tokens),i) * np.log((self.total_df - 1 + 0.5)/ (1 + 0.5))\n",
    "#             print(v_tf_part.shape, type(v_tf_part))\n",
    "#             print(v_idf_q.reshape(-1).shape, type(v_idf_q))\n",
    "#             print(v_mid.shape)\n",
    "            scores = (np.asarray(v_tf_part) * v_idf_q.reshape(-1)).dot(v_mid)\n",
    "            result.append(np.log(np.maximum(scores, 1.)))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = MyExtractor(dtm, idx2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-cae238355b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbm25\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-bdc86124449f>\u001b[0m in \u001b[0;36mbm25\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#             print(v_idf_q.reshape(-1).shape, type(v_idf_q))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#             print(v_mid.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_tf_part\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv_idf_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf = me.bm25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.broadcast_arrays()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(1, 10,k=1).reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Fit_transform tfidf vector with normalization\n",
    "    tfidf = TfidfTransformer(norm=False)\n",
    "    dtm = tfidf.fit_transform(dtm)\n",
    "    scores = np.squeeze(np.asarray(dtm.sum(axis=0))) # document-wise sum\n",
    "    \n",
    "    # 파레토 법칙ㅋㅋㅋ 상위 20%가 전체 컨텐츠의 80%를 대변함.\n",
    "    if top_k is None:\n",
    "        top_k = int(len(scores) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-50e342ee6580>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dtm' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-29c4a0afd4b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.80482526, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 8.80482526, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 8.80482526, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 8.80482526, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 8.80482526,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        8.80482526]])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.80482526, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 8.80482526, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 8.80482526, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 8.80482526, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 8.80482526,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        8.80482526]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.maximum(tf.toarray(), 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 3., 1., 2.])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(np.array([1.,3., 0.5, 2]), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "loop of ufunc does not support argument 0 of type dia_matrix which has no callable log method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: log not found",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-300-7b73ba95910c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type dia_matrix which has no callable log method"
     ]
    }
   ],
   "source": [
    "np.log(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.maximum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1 stored elements (1 diagonals) in DIAgonal format>"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999.5"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(me.total_df - 1 + 0.5) / 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = dtm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.sum(axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [2, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x19751 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 570380 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method MyExtractor.bm25 of <__main__.MyExtractor object at 0x7f2ffedc2f10>>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x19751 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 570380 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.toarray()[279,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-7cea4ba7c034>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassicExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_terms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_terms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_doc_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_doc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-7bb05c6dcea7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, query_terms, doc_terms, df, total_df, avg_doc_len)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mavg_doc_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0mof\u001b[0m \u001b[0mavg\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \"\"\"\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mquery_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquery_terms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mquery_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdoc_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "extractor = ClassicExtractor(query_terms, docs_terms[201], df, total_df, avg_doc_len=avg_doc_len)\n",
    "extractor.get_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm[:,token2idx['약산성']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['약산성']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['세정력']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 if '약산성' in counts else 0 for _, counts in docs_terms.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([counts.get('약산성', 0) for _, counts in docs_terms.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1 if '세정력' in counts else 0 for _, counts in docs_terms.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['세정력']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.7427"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_doc_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '샤넬 립 괜찮은듯.'\n",
    "query_terms, query_len = text2lm(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'약산성': 54,\n",
       " '이': 7351,\n",
       " '라': 1945,\n",
       " '순': 58,\n",
       " '하': 6485,\n",
       " '긴': 886,\n",
       " '한데': 460,\n",
       " '세정력': 169,\n",
       " '엄청': 726,\n",
       " '좋': 5977,\n",
       " '은': 5430,\n",
       " '건': 744,\n",
       " '아니': 1170,\n",
       " '에요': 1126,\n",
       " 'ㅎㅎ': 700,\n",
       " '향': 1634,\n",
       " '무': 230,\n",
       " '고': 7601,\n",
       " '닦': 262,\n",
       " '폼': 131,\n",
       " '클렌징': 353,\n",
       " '으로': 3088,\n",
       " '2': 641,\n",
       " '차': 145,\n",
       " '세안': 243,\n",
       " '필수': 94,\n",
       " '입': 252,\n",
       " '니당': 186,\n",
       " '!': 3728,\n",
       " '워터': 197,\n",
       " '만': 2009,\n",
       " '쓰': 3148,\n",
       " '시': 824,\n",
       " '는': 7066,\n",
       " '분': 1095,\n",
       " '들': 2487,\n",
       " '께': 138,\n",
       " '비추': 111,\n",
       " '에': 5890,\n",
       " '요': 1253,\n",
       " '~': 769,\n",
       " '그렇': 302,\n",
       " '다고': 769,\n",
       " '안': 2858,\n",
       " '바이': 13,\n",
       " '오더': 5,\n",
       " '마': 91,\n",
       " '보다': 1394,\n",
       " '잔여물': 36,\n",
       " '계속': 391,\n",
       " '남': 535,\n",
       " '느낌': 2031,\n",
       " '입니다': 1718,\n",
       " '아주': 413,\n",
       " '잘': 3235,\n",
       " '있': 4225,\n",
       " '다': 2497,\n",
       " '.': 5573,\n",
       " '색연필': 3,\n",
       " '깍': 7,\n",
       " '기': 2705,\n",
       " '^^': 134,\n",
       " '이거': 1453,\n",
       " '로': 2359,\n",
       " '맥퀸': 2,\n",
       " '뉴욕': 3,\n",
       " '아이라이너': 66,\n",
       " '으면': 537,\n",
       " '뭉개': 7,\n",
       " '짐': 126,\n",
       " '장난아': 5,\n",
       " '님': 80,\n",
       " '..': 1253,\n",
       " '나': 2580,\n",
       " '의': 1524,\n",
       " '컬러': 434,\n",
       " '링북': 1,\n",
       " '을': 3997,\n",
       " '책임질': 1,\n",
       " '아이': 343,\n",
       " '#': 356,\n",
       " '데일리': 211,\n",
       " '템': 440,\n",
       " '헤어': 122,\n",
       " '팩': 555,\n",
       " '아서': 1759,\n",
       " '사용': 2867,\n",
       " '이것': 253,\n",
       " '면': 3622,\n",
       " '머릿결': 128,\n",
       " '부드러워': 80,\n",
       " '지': 3758,\n",
       " '지성': 380,\n",
       " '두': 562,\n",
       " '피': 272,\n",
       " '자주': 321,\n",
       " '그래서': 512,\n",
       " '가끔': 164,\n",
       " '씩': 323,\n",
       " '중': 899,\n",
       " '깨끗히': 12,\n",
       " '감기': 18,\n",
       " '것': 2475,\n",
       " '같': 3296,\n",
       " '머리카락': 112,\n",
       " '뻑뻑': 104,\n",
       " '건조': 1035,\n",
       " '게': 4768,\n",
       " '느껴짐': 24,\n",
       " 'ㅠㅠ': 751,\n",
       " '상한': 15,\n",
       " '머리': 400,\n",
       " '정말': 986,\n",
       " 'ㅠ': 353,\n",
       " '심한': 83,\n",
       " '이나': 559,\n",
       " '그런': 482,\n",
       " '듯': 1043,\n",
       " '하나': 651,\n",
       " '도': 5664,\n",
       " '뽑혀': 9,\n",
       " '자극': 565,\n",
       " '심하': 174,\n",
       " '모공': 353,\n",
       " '털': 74,\n",
       " '뽑혀서': 5,\n",
       " '더': 1536,\n",
       " '도드라졌': 1,\n",
       " '어요': 3415,\n",
       " '크림': 867,\n",
       " '예방': 33,\n",
       " '차원': 10,\n",
       " '작년': 18,\n",
       " '잡': 293,\n",
       " '부록': 1,\n",
       " '쟁여': 124,\n",
       " '논': 6,\n",
       " '거': 2161,\n",
       " '는데': 4128,\n",
       " '찾': 290,\n",
       " '힘들': 150,\n",
       " '음': 1605,\n",
       " 'ㅜㅜ': 217,\n",
       " '제품': 2834,\n",
       " '짱짱': 181,\n",
       " '지만': 2190,\n",
       " '가격': 1255,\n",
       " '최악': 53,\n",
       " '^^...': 3,\n",
       " '굿': 234,\n",
       " '태': 10,\n",
       " '닝': 141,\n",
       " '냄새': 585,\n",
       " '때문': 529,\n",
       " '되': 2655,\n",
       " '되게': 219,\n",
       " '많': 873,\n",
       " '은데': 640,\n",
       " '얘': 272,\n",
       " '가': 4620,\n",
       " '아요': 3358,\n",
       " '바르': 1949,\n",
       " '아빠': 30,\n",
       " '어디': 104,\n",
       " '서': 1703,\n",
       " '자꾸': 70,\n",
       " '단내': 3,\n",
       " '난다': 32,\n",
       " '그래요': 45,\n",
       " '달콤': 40,\n",
       " '한': 4190,\n",
       " '코코넛': 22,\n",
       " 'ㅎ': 231,\n",
       " '저': 1658,\n",
       " '워낙': 102,\n",
       " '타': 78,\n",
       " '그런지': 302,\n",
       " '랑': 649,\n",
       " '인': 1246,\n",
       " '텐': 14,\n",
       " '파이어': 2,\n",
       " '를': 1933,\n",
       " '같이': 460,\n",
       " '써야': 123,\n",
       " '뭔가': 293,\n",
       " '쫌': 53,\n",
       " '더군요': 24,\n",
       " '그래도': 519,\n",
       " '피부': 2403,\n",
       " '보호': 59,\n",
       " '해': 2032,\n",
       " '주': 1890,\n",
       " '자외선': 68,\n",
       " '차단': 63,\n",
       " '니까': 544,\n",
       " '케이스': 226,\n",
       " '넘': 402,\n",
       " '귀엽': 32,\n",
       " '향기': 194,\n",
       " '제': 1036,\n",
       " '면서': 837,\n",
       " '았': 1953,\n",
       " '던': 977,\n",
       " '점': 516,\n",
       " '아프': 76,\n",
       " '않': 2941,\n",
       " '향긋': 15,\n",
       " '뿐': 101,\n",
       " '었': 1683,\n",
       " '속': 315,\n",
       " '수분': 704,\n",
       " '부족': 188,\n",
       " '건성': 443,\n",
       " '타입': 505,\n",
       " '에게': 351,\n",
       " '감': 1179,\n",
       " '보습': 617,\n",
       " '그': 719,\n",
       " '무엇': 175,\n",
       " '만족': 629,\n",
       " '시켜': 149,\n",
       " '못했': 83,\n",
       " '습니다': 1480,\n",
       " 'ㅜㅜ주르륵': 1,\n",
       " '흐르': 77,\n",
       " '100': 81,\n",
       " '%': 114,\n",
       " '세': 377,\n",
       " '럼': 153,\n",
       " '.?': 137,\n",
       " '갸웃거리': 1,\n",
       " '했': 1880,\n",
       " '네요': 1248,\n",
       " '조금': 802,\n",
       " '라도': 74,\n",
       " '신': 284,\n",
       " '추천': 843,\n",
       " '드리': 62,\n",
       " '싶': 780,\n",
       " '가지': 400,\n",
       " '번들거림': 22,\n",
       " '없이': 606,\n",
       " '산뜻': 230,\n",
       " '느껴서': 12,\n",
       " '실': 205,\n",
       " '아쉽': 166,\n",
       " '맞': 625,\n",
       " 'ㅜㅜ사용후': 1,\n",
       " '가려움': 22,\n",
       " ',': 1833,\n",
       " '따가움': 32,\n",
       " '없': 2635,\n",
       " '까지': 699,\n",
       " '.!': 70,\n",
       " '무상': 876,\n",
       " '제공': 889,\n",
       " '받': 1384,\n",
       " '아': 2647,\n",
       " '후': 1321,\n",
       " '솔직': 886,\n",
       " '작성': 887,\n",
       " '된': 1241,\n",
       " '리뷰': 1027,\n",
       " '너무': 2222,\n",
       " '순한': 139,\n",
       " '인가': 110,\n",
       " '....': 118,\n",
       " '복합': 140,\n",
       " '성': 942,\n",
       " '웁': 3,\n",
       " '니': 925,\n",
       " '세수': 75,\n",
       " '시간': 457,\n",
       " '돼서': 125,\n",
       " '기름': 175,\n",
       " '올라오': 246,\n",
       " '부드럽': 503,\n",
       " '할': 1628,\n",
       " '수': 1493,\n",
       " '어서': 1820,\n",
       " '제대로': 74,\n",
       " '건지': 155,\n",
       " '의심': 16,\n",
       " '됩니다': 160,\n",
       " 'ㅠ기름기가': 1,\n",
       " '제거': 285,\n",
       " '뾰루지': 76,\n",
       " '올라왔': 50,\n",
       " '원래': 343,\n",
       " '약간': 652,\n",
       " '처음': 712,\n",
       " '쓸': 659,\n",
       " '땐': 174,\n",
       " '해서': 1915,\n",
       " '썼': 471,\n",
       " '커': 363,\n",
       " '버력': 333,\n",
       " '포기': 27,\n",
       " '더라구요': 781,\n",
       " '좀': 1600,\n",
       " '많이': 1162,\n",
       " '발라도': 35,\n",
       " '얼굴': 838,\n",
       " '귀신': 6,\n",
       " '처럼': 678,\n",
       " '진짜': 1412,\n",
       " '살짝': 531,\n",
       " '양': 582,\n",
       " '볼': 259,\n",
       " '이랑': 440,\n",
       " '이마': 71,\n",
       " '코': 287,\n",
       " '턱': 57,\n",
       " '뭍': 28,\n",
       " '혀': 28,\n",
       " '준': 105,\n",
       " '다음': 372,\n",
       " '퍼프': 221,\n",
       " '마구': 9,\n",
       " '두드리': 38,\n",
       " '나름': 108,\n",
       " '촉촉': 1547,\n",
       " '완성': 39,\n",
       " '돼요': 139,\n",
       " '강추': 166,\n",
       " '...♡': 1,\n",
       " '돈': 190,\n",
       " '맨날': 70,\n",
       " '사': 1049,\n",
       " '펑펑': 5,\n",
       " '해외': 11,\n",
       " '나갈': 35,\n",
       " '일': 708,\n",
       " '최대한': 19,\n",
       " '놓': 205,\n",
       " '버': 24,\n",
       " '다릅니다': 6,\n",
       " '떡': 89,\n",
       " '지지': 38,\n",
       " '자연': 572,\n",
       " '스럽': 511,\n",
       " '져요': 134,\n",
       " '인위': 51,\n",
       " '적': 1616,\n",
       " '호불호': 127,\n",
       " '일단': 575,\n",
       " '색깔': 162,\n",
       " '회색': 22,\n",
       " '구분': 11,\n",
       " '가능': 269,\n",
       " '쾌감': 7,\n",
       " '업': 178,\n",
       " '!!': 325,\n",
       " 'ㅋㅋㅋ': 400,\n",
       " 'ㅋㅋ': 564,\n",
       " '과잉': 5,\n",
       " '피지': 180,\n",
       " '용': 611,\n",
       " '구매': 888,\n",
       " '꽤': 226,\n",
       " '필요': 291,\n",
       " '걷어가': 1,\n",
       " '톡톡': 75,\n",
       " '해줘도': 8,\n",
       " '극': 96,\n",
       " '느낄': 73,\n",
       " '대비': 210,\n",
       " '가성': 136,\n",
       " '비': 224,\n",
       " '능력': 11,\n",
       " '수치': 1,\n",
       " '높': 130,\n",
       " '다시': 308,\n",
       " '렵니다': 6,\n",
       " '밤': 323,\n",
       " '자': 321,\n",
       " '담날': 33,\n",
       " '진': 631,\n",
       " '느껴져요': 50,\n",
       " '사요': 50,\n",
       " '인데': 1215,\n",
       " '날': 506,\n",
       " '해요': 1080,\n",
       " '*': 138,\n",
       " '평가': 296,\n",
       " '단': 348,\n",
       " '02': 26,\n",
       " '호': 404,\n",
       " '오드': 26,\n",
       " '로즈': 90,\n",
       " '자글자글': 15,\n",
       " '펄': 185,\n",
       " '연한': 48,\n",
       " '분홍': 7,\n",
       " '색감': 104,\n",
       " '예요': 303,\n",
       " '생각': 980,\n",
       " '펄이': 66,\n",
       " '작': 302,\n",
       " '입자': 95,\n",
       " '눈': 586,\n",
       " '밑': 101,\n",
       " '려': 34,\n",
       " '했으나': 13,\n",
       " '실망': 55,\n",
       " '덩이': 19,\n",
       " '발라': 767,\n",
       " '크': 363,\n",
       " '티': 197,\n",
       " '몇': 311,\n",
       " '보': 1745,\n",
       " '밀착력': 200,\n",
       " '은은': 337,\n",
       " '어떻게': 56,\n",
       " '할까': 24,\n",
       " '다가': 637,\n",
       " '베이스': 331,\n",
       " '먼저': 78,\n",
       " '가루': 204,\n",
       " '섀도우': 122,\n",
       " '발색': 490,\n",
       " '력': 420,\n",
       " '끝': 186,\n",
       " '포인트': 84,\n",
       " '전': 850,\n",
       " '올라와서': 47,\n",
       " '다른': 935,\n",
       " '색상': 498,\n",
       " '써': 1146,\n",
       " '봐서': 66,\n",
       " '모르': 763,\n",
       " '겠': 1176,\n",
       " '23': 71,\n",
       " '그다지': 25,\n",
       " '프라이머': 86,\n",
       " '몸': 126,\n",
       " '로션': 333,\n",
       " '꼭': 302,\n",
       " '줘야': 140,\n",
       " '합': 70,\n",
       " 'ㅠㅠ특히': 3,\n",
       " '등': 182,\n",
       " '드름': 29,\n",
       " '심해서': 47,\n",
       " '피부과': 48,\n",
       " '에서': 1291,\n",
       " '파': 366,\n",
       " '재생': 13,\n",
       " '써요': 104,\n",
       " '비싸': 256,\n",
       " '성분': 728,\n",
       " '착한': 34,\n",
       " '이게': 322,\n",
       " '순하': 202,\n",
       " '호주': 3,\n",
       " '국민': 5,\n",
       " '라길래': 23,\n",
       " '봤': 997,\n",
       " '딱히': 251,\n",
       " '뭐': 387,\n",
       " '난다거나': 9,\n",
       " '구요': 506,\n",
       " '바를': 385,\n",
       " '예정': 81,\n",
       " '제형': 766,\n",
       " '연고': 27,\n",
       " '끈적거리': 61,\n",
       " '유분기': 135,\n",
       " '파운데이션': 166,\n",
       " '주위': 32,\n",
       " '발랐': 288,\n",
       " '는데요': 154,\n",
       " '각질': 549,\n",
       " '생기': 198,\n",
       " '괜찮': 1013,\n",
       " '전체': 190,\n",
       " '보단': 200,\n",
       " '부분': 463,\n",
       " '걸': 424,\n",
       " '할께요': 3,\n",
       " '평': 124,\n",
       " '싼': 71,\n",
       " '편': 1026,\n",
       " '부들부들': 127,\n",
       " '곳': 116,\n",
       " '막': 370,\n",
       " '보여요': 81,\n",
       " '저금': 1,\n",
       " '립스틱': 151,\n",
       " 'ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ': 1,\n",
       " '부각': 188,\n",
       " '보송': 107,\n",
       " '깔끔': 229,\n",
       " '올라가': 92,\n",
       " '이쁘': 195,\n",
       " '맘': 282,\n",
       " '!!!!!!': 9,\n",
       " '어': 2238,\n",
       " '후기': 192,\n",
       " '또': 392,\n",
       " '구입': 174,\n",
       " '트러블': 536,\n",
       " '유발': 34,\n",
       " '합니다': 745,\n",
       " '참고': 85,\n",
       " '민감': 428,\n",
       " '장시간': 21,\n",
       " '메이크업': 314,\n",
       " '뒤': 193,\n",
       " '당김': 109,\n",
       " '느껴졌': 64,\n",
       " '용량': 270,\n",
       " '대신': 179,\n",
       " '임': 468,\n",
       " '평소': 242,\n",
       " '더페이스샵': 15,\n",
       " '차단제': 11,\n",
       " '특유': 102,\n",
       " '기름기': 49,\n",
       " '돌': 136,\n",
       " '화장': 770,\n",
       " '밀리': 130,\n",
       " '었었': 12,\n",
       " '그러': 164,\n",
       " '와중': 10,\n",
       " '글': 215,\n",
       " '로우': 74,\n",
       " '픽': 132,\n",
       " '덕분': 51,\n",
       " '근데': 863,\n",
       " '왠걸': 6,\n",
       " '내': 642,\n",
       " '썬': 58,\n",
       " '였': 501,\n",
       " '봄': 160,\n",
       " '알': 459,\n",
       " '흡수': 531,\n",
       " '특히': 199,\n",
       " '백': 100,\n",
       " '탁': 5,\n",
       " '현': 3,\n",
       " '상': 74,\n",
       " '절대': 97,\n",
       " '!!!!!!!!!!': 3,\n",
       " '현상': 103,\n",
       " '뭔가요': 3,\n",
       " '?': 1218,\n",
       " '먹': 384,\n",
       " '건가': 52,\n",
       " '여': 356,\n",
       " '!!!': 116,\n",
       " '발림': 476,\n",
       " '환절기': 50,\n",
       " '겨울': 387,\n",
       " '딱': 675,\n",
       " '게다가': 56,\n",
       " '어찌나': 2,\n",
       " '발리': 387,\n",
       " '인지': 171,\n",
       " '우리': 49,\n",
       " '선크림': 194,\n",
       " '아님': 85,\n",
       " '찾아보': 45,\n",
       " '정도': 1213,\n",
       " '가치': 12,\n",
       " '에센스': 352,\n",
       " '향도': 231,\n",
       " '보들보들': 98,\n",
       " '해져서': 116,\n",
       " '끈적': 75,\n",
       " '지나': 276,\n",
       " '보들': 12,\n",
       " '신기': 154,\n",
       " '두두둑': 1,\n",
       " '떨어져요': 24,\n",
       " 'ㅠㅠㅠ': 124,\n",
       " '아쉬웠': 94,\n",
       " '하지만': 308,\n",
       " '저녁': 130,\n",
       " '스킨케어': 84,\n",
       " '도착': 3,\n",
       " '한참': 24,\n",
       " '썻었는데': 4,\n",
       " '이제': 128,\n",
       " 'ㅜㅠ': 22,\n",
       " '효과': 1115,\n",
       " '오래': 430,\n",
       " '기존': 91,\n",
       " '마다가스카': 1,\n",
       " '라인': 269,\n",
       " '언니': 67,\n",
       " '자고': 4,\n",
       " '심사숙고': 1,\n",
       " '샀': 634,\n",
       " '습': 14,\n",
       " '미': 160,\n",
       " '오일': 367,\n",
       " '라고': 480,\n",
       " '셔서': 73,\n",
       " '역시': 171,\n",
       " '걱정': 189,\n",
       " '무색': 5,\n",
       " '악악': 3,\n",
       " '충분': 68,\n",
       " '단독': 88,\n",
       " '기름지': 79,\n",
       " '가볍': 347,\n",
       " '번들거리': 42,\n",
       " '팡팡': 26,\n",
       " '어유': 3,\n",
       " '귀찮': 167,\n",
       " '아침': 347,\n",
       " '둘': 101,\n",
       " '번': 1111,\n",
       " '충분히': 99,\n",
       " '드': 278,\n",
       " '네': 247,\n",
       " '엥': 9,\n",
       " '갈리': 29,\n",
       " '컨': 182,\n",
       " '실러': 162,\n",
       " '쿠션': 212,\n",
       " '쫀쫀': 228,\n",
       " '두텁': 6,\n",
       " '커버': 254,\n",
       " '표현': 276,\n",
       " '매트': 304,\n",
       " '그리고': 800,\n",
       " '크기': 111,\n",
       " '아담': 4,\n",
       " '외출용': 2,\n",
       " '다니': 208,\n",
       " '간편': 100,\n",
       " '외관': 24,\n",
       " '고급': 119,\n",
       " '스러워': 96,\n",
       " '단점': 450,\n",
       " '라면': 170,\n",
       " '두드려야': 3,\n",
       " '된다는': 54,\n",
       " '딴': 29,\n",
       " '두드려서': 16,\n",
       " '펴': 172,\n",
       " '핑크': 311,\n",
       " '끼': 273,\n",
       " '톤': 433,\n",
       " '사람': 330,\n",
       " '굉장히': 246,\n",
       " '떠': 26,\n",
       " '보일': 29,\n",
       " '다는': 592,\n",
       " '화사': 130,\n",
       " '쬐': 6,\n",
       " '끄': 11,\n",
       " '망한': 2,\n",
       " '삼': 74,\n",
       " '원': 428,\n",
       " '야': 169,\n",
       " '무난': 513,\n",
       " '끈': 187,\n",
       " '쫙': 33,\n",
       " '달라붙': 54,\n",
       " '습니당': 156,\n",
       " '화해': 10,\n",
       " '등록': 53,\n",
       " '구연산': 1,\n",
       " '소듐': 3,\n",
       " '벤조': 1,\n",
       " '에이트': 1,\n",
       " '포타슘': 1,\n",
       " '소르': 2,\n",
       " '베이트': 1,\n",
       " '시트': 150,\n",
       " '레이트': 4,\n",
       " '제외': 48,\n",
       " '됨': 246,\n",
       " '알로에': 47,\n",
       " '베라': 8,\n",
       " '추출물': 84,\n",
       " '글리세린': 6,\n",
       " '손': 546,\n",
       " '씻': 250,\n",
       " '똑같': 53,\n",
       " '즉': 6,\n",
       " '체감': 9,\n",
       " '덜': 351,\n",
       " '핸드': 20,\n",
       " '워시': 73,\n",
       " '거품': 324,\n",
       " '나와서': 102,\n",
       " '따로': 171,\n",
       " '편리': 76,\n",
       " '250': 2,\n",
       " '/': 92,\n",
       " '500': 19,\n",
       " '미리': 35,\n",
       " '사이즈': 92,\n",
       " '나오': 241,\n",
       " '리필': 32,\n",
       " '살': 358,\n",
       " '파우더': 293,\n",
       " '청포': 10,\n",
       " '도향': 9,\n",
       " '상당히': 30,\n",
       " '향료': 42,\n",
       " '낸': 24,\n",
       " '지속': 133,\n",
       " '불': 76,\n",
       " '호고': 1,\n",
       " '못': 581,\n",
       " '드림': 16,\n",
       " '물': 511,\n",
       " '탄': 15,\n",
       " '??': 28,\n",
       " '클렌': 81,\n",
       " '으니까': 50,\n",
       " '신경': 135,\n",
       " '쓰임': 4,\n",
       " '-': 503,\n",
       " '특허': 14,\n",
       " '초음파': 4,\n",
       " '공법': 11,\n",
       " '60': 12,\n",
       " '저온': 3,\n",
       " '13': 53,\n",
       " '이상': 258,\n",
       " '직접': 58,\n",
       " '달인': 1,\n",
       " '천궁': 1,\n",
       " '쑥': 29,\n",
       " '과': 851,\n",
       " '황토': 2,\n",
       " '참숯': 2,\n",
       " '기분': 346,\n",
       " '맑': 117,\n",
       " '집니다': 64,\n",
       " '순면': 28,\n",
       " '쉬운': 26,\n",
       " '포근': 31,\n",
       " '부드러운': 151,\n",
       " '감촉': 15,\n",
       " '편안': 92,\n",
       " '함': 1039,\n",
       " '줍니다': 138,\n",
       " '120': 7,\n",
       " '년': 205,\n",
       " '전통': 1,\n",
       " '면직': 1,\n",
       " '제조': 7,\n",
       " '기술': 14,\n",
       " '짠': 12,\n",
       " '미세': 66,\n",
       " '모눈': 1,\n",
       " '패턴': 11,\n",
       " '분비물': 7,\n",
       " '빠른': 46,\n",
       " '도와': 47,\n",
       " '언제나': 7,\n",
       " '보송보송': 66,\n",
       " '통기성': 4,\n",
       " '습한': 7,\n",
       " '공기': 15,\n",
       " '순환': 2,\n",
       " '착용감': 95,\n",
       " '더블': 26,\n",
       " '번짐': 53,\n",
       " '생리': 47,\n",
       " '혈': 10,\n",
       " '안심': 56,\n",
       " '0': 65,\n",
       " 'N': 10,\n",
       " '붉': 181,\n",
       " '노랗': 28,\n",
       " '밝': 204,\n",
       " '상아색': 5,\n",
       " '찍': 63,\n",
       " '이미': 31,\n",
       " '싱이': 8,\n",
       " '싱': 34,\n",
       " '빠르': 140,\n",
       " '눈가': 76,\n",
       " '뜨': 208,\n",
       " '된다': 68,\n",
       " '굳이': 115,\n",
       " '가장': 212,\n",
       " '마음': 310,\n",
       " '얇': 483,\n",
       " '다크': 115,\n",
       " '요즘': 337,\n",
       " '코로나': 24,\n",
       " '인해': 30,\n",
       " '마스크': 381,\n",
       " '종종': 31,\n",
       " '썼었': 68,\n",
       " '무너짐': 62,\n",
       " '오랜': 31,\n",
       " '아직': 191,\n",
       " '미숙': 3,\n",
       " '해서인지': 8,\n",
       " '비해서': 50,\n",
       " '그렇게': 252,\n",
       " '은지': 73,\n",
       " '페': 6,\n",
       " '잉크': 4,\n",
       " '데나': 5,\n",
       " '클리오': 31,\n",
       " '블러': 152,\n",
       " '셔': 151,\n",
       " '와': 492,\n",
       " '쉐': 100,\n",
       " '딩': 155,\n",
       " '블': 86,\n",
       " '렌': 61,\n",
       " '치크': 44,\n",
       " '&': 86,\n",
       " '브러쉬': 369,\n",
       " '1': 678,\n",
       " '섬세': 49,\n",
       " '터치': 44,\n",
       " 'UP': 12,\n",
       " '인조모': 7,\n",
       " '재질': 37,\n",
       " '예민': 223,\n",
       " '조절': 250,\n",
       " '사선': 10,\n",
       " '컷팅': 9,\n",
       " '모양': 120,\n",
       " '부위': 107,\n",
       " '정교': 12,\n",
       " '연출': 109,\n",
       " '손쉽': 8,\n",
       " '입술': 318,\n",
       " '트': 231,\n",
       " '란': 44,\n",
       " '말': 483,\n",
       " '들어가': 188,\n",
       " '어도': 128,\n",
       " '바로': 345,\n",
       " '구': 373,\n",
       " '매각': 5,\n",
       " '..^^': 5,\n",
       " '이렇게': 194,\n",
       " '뜯': 65,\n",
       " '바빴': 2,\n",
       " '악순환': 2,\n",
       " '반복': 9,\n",
       " '저렴': 397,\n",
       " '로써': 24,\n",
       " '짜': 140,\n",
       " '난': 278,\n",
       " '자기': 106,\n",
       " '그대로': 164,\n",
       " '때': 2405,\n",
       " '해져': 19,\n",
       " '발견': 33,\n",
       " '적당': 220,\n",
       " '안정': 32,\n",
       " '직': 13,\n",
       " '빵': 15,\n",
       " '인듯': 28,\n",
       " '서지인': 1,\n",
       " '선물': 215,\n",
       " '알약': 15,\n",
       " '안성맞춤': 9,\n",
       " '늦': 24,\n",
       " '자서': 3,\n",
       " '피곤': 17,\n",
       " '이걸': 172,\n",
       " '꾸준히': 218,\n",
       " '챙겨': 45,\n",
       " '피로': 14,\n",
       " '덜하': 26,\n",
       " '푸석푸석': 21,\n",
       " '거나': 527,\n",
       " '어두운': 73,\n",
       " '덜해진': 1,\n",
       " '고요': 142,\n",
       " '코팅': 54,\n",
       " '단종': 79,\n",
       " '새로운': 24,\n",
       " '버전': 37,\n",
       " '그레이': 38,\n",
       " '브라운': 148,\n",
       " '아이템': 60,\n",
       " '떨어트리': 5,\n",
       " '깨': 32,\n",
       " '지기': 3,\n",
       " '눈썹': 128,\n",
       " '양쪽': 14,\n",
       " '섞': 138,\n",
       " '스러운': 238,\n",
       " '동동': 17,\n",
       " '보인다는': 10,\n",
       " '손등': 73,\n",
       " '테스트': 133,\n",
       " '세요': 275,\n",
       " '떨': 11,\n",
       " '렸': 8,\n",
       " '진심': 41,\n",
       " '쪼개': 3,\n",
       " '져서': 123,\n",
       " '난감': 5,\n",
       " '햇': 52,\n",
       " '어휴': 1,\n",
       " '줄': 377,\n",
       " '앞': 102,\n",
       " '걸요': 3,\n",
       " '뭉치': 132,\n",
       " '살살': 55,\n",
       " '풀': 126,\n",
       " '지워': 249,\n",
       " '유용': 43,\n",
       " '브로우': 64,\n",
       " '마스터': 4,\n",
       " '아이브': 5,\n",
       " '키트': 5,\n",
       " '왜': 264,\n",
       " '케': 68,\n",
       " '만드셨': 2,\n",
       " '나요': 63,\n",
       " '뚜껑': 90,\n",
       " '올려서': 8,\n",
       " '..(': 8,\n",
       " '소량': 82,\n",
       " '.)': 19,\n",
       " '하얗': 77,\n",
       " '죠': 174,\n",
       " '21': 118,\n",
       " '이건': 456,\n",
       " '거의': 355,\n",
       " '17': 24,\n",
       " '코끼': 3,\n",
       " '주변': 106,\n",
       " '그런데': 176,\n",
       " '만약': 12,\n",
       " '올린다면': 1,\n",
       " '게이샤': 1,\n",
       " '낮': 70,\n",
       " '뽀드득': 8,\n",
       " '좋아하': 109,\n",
       " '신다면': 32,\n",
       " '클린': 19,\n",
       " '으십니다': 2,\n",
       " '씻긴': 5,\n",
       " '여러': 213,\n",
       " '미끄덩': 33,\n",
       " '토너': 212,\n",
       " '묻': 248,\n",
       " '질': 139,\n",
       " '정확': 16,\n",
       " '자마자': 134,\n",
       " '결론': 55,\n",
       " '부터': 328,\n",
       " '얘기': 36,\n",
       " '자면': 90,\n",
       " '졌어요': 29,\n",
       " '수축': 11,\n",
       " '건진': 9,\n",
       " '블랙': 157,\n",
       " '헤드': 95,\n",
       " '개선': 141,\n",
       " '보이': 383,\n",
       " '여드름': 303,\n",
       " '한번': 150,\n",
       " '줬': 143,\n",
       " '땡기': 43,\n",
       " '사실': 216,\n",
       " '뒤집': 80,\n",
       " '어쩌': 17,\n",
       " '됬': 52,\n",
       " '길': 266,\n",
       " '정확히': 10,\n",
       " '주정': 2,\n",
       " '썻는데': 20,\n",
       " '사이': 73,\n",
       " '확': 82,\n",
       " '더라고요': 239,\n",
       " '다만': 345,\n",
       " '케어': 227,\n",
       " '위해': 60,\n",
       " '엔': 578,\n",
       " '아까운': 16,\n",
       " 'ㅠㅠ아참': 1,\n",
       " '상처': 28,\n",
       " '첨': 95,\n",
       " '따가운': 40,\n",
       " '데': 1085,\n",
       " '붉어지': 11,\n",
       " '주의': 31,\n",
       " '세영': 2,\n",
       " '~~': 154,\n",
       " '가단': 85,\n",
       " '볼려고': 4,\n",
       " '원하': 54,\n",
       " '한테': 312,\n",
       " '할게요': 21,\n",
       " '👍👍': 30,\n",
       " '광택': 53,\n",
       " '마르': 98,\n",
       " '속도': 16,\n",
       " ...}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ClassicExtractor(query_terms, docs_terms[201], df, total_df, avg_doc_len=avg_doc_len)\n",
    "extractor.get_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'설화수': 1, '비싸': 1, '고': 1, '별로': 1, '에요': 1, '.': 1}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'엄마 한테 생일 선물로 사줬는데 일단 지속력 합격 , 발색 좋음 !! 색깔이 너무 맘에 들었어요 연한 핑크 장미색에 코랄을 살짝 찍어 넣은 듯한 그런 부드럽고 은은하면서 귀여운 느낌이랄까 역시 샤넬은 어떤 색상을 사용해도 다 찰떡같아요 ! 쵝호'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[201]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'설': 1,\n",
       " '페이드': 1,\n",
       " '없': 1,\n",
       " '는': 6,\n",
       " '샴푸': 3,\n",
       " '찾': 1,\n",
       " '다가': 1,\n",
       " '애용': 1,\n",
       " '하': 2,\n",
       " '던': 1,\n",
       " '시드': 1,\n",
       " '물': 3,\n",
       " '에서': 1,\n",
       " '발견': 1,\n",
       " '고': 8,\n",
       " '샀': 1,\n",
       " '음': 4,\n",
       " '!': 2,\n",
       " '이전': 1,\n",
       " '에': 2,\n",
       " '려': 1,\n",
       " '썼': 1,\n",
       " '는데': 2,\n",
       " '천연': 1,\n",
       " '이': 3,\n",
       " '아니': 1,\n",
       " '라': 1,\n",
       " '세정력': 1,\n",
       " '은': 2,\n",
       " '좋': 3,\n",
       " '았': 1,\n",
       " '지만': 1,\n",
       " '향': 2,\n",
       " '역겹': 1,\n",
       " '불편': 1,\n",
       " '했': 1,\n",
       " '게다가': 1,\n",
       " '청색': 1,\n",
       " '1': 2,\n",
       " '호': 1,\n",
       " '들어갔': 1,\n",
       " 'ㅜㅜ근데': 1,\n",
       " '이건': 2,\n",
       " '성분': 1,\n",
       " '도': 2,\n",
       " '사용': 1,\n",
       " '감': 3,\n",
       " '비누': 1,\n",
       " '로': 5,\n",
       " '듯이': 1,\n",
       " '뻑뻑': 1,\n",
       " '함': 4,\n",
       " '기름지': 1,\n",
       " '지': 1,\n",
       " '않': 2,\n",
       " '산뜻': 1,\n",
       " '그리고': 1,\n",
       " '아토피': 1,\n",
       " '있': 1,\n",
       " '동생': 1,\n",
       " '맨날': 1,\n",
       " '머리': 2,\n",
       " '긁': 1,\n",
       " '나서': 1,\n",
       " '안': 1,\n",
       " '가렵': 1,\n",
       " '다고': 1,\n",
       " '<': 1,\n",
       " '확실': 1,\n",
       " '치': 1,\n",
       " '으니': 1,\n",
       " '며칠': 1,\n",
       " '더': 2,\n",
       " '써': 1,\n",
       " '보': 1,\n",
       " '추가': 1,\n",
       " '미지근': 1,\n",
       " '한': 2,\n",
       " '두피': 1,\n",
       " '모공': 1,\n",
       " '열': 1,\n",
       " '차': 2,\n",
       " '조금': 1,\n",
       " '다음': 1,\n",
       " '헹구': 1,\n",
       " '2': 1,\n",
       " '거품': 1,\n",
       " '내주': 1,\n",
       " '면': 1,\n",
       " '세정': 1,\n",
       " '잘': 1,\n",
       " '됨': 1}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_terms[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lm': -32.845013833906705,\n",
       " 'lm_dir': -2.7215124848325485,\n",
       " 'lm_jm': -21.689076639689407,\n",
       " 'lm_twoway': -16.295780807057437,\n",
       " 'bm25': 0.0,\n",
       " 'coordinate': 1,\n",
       " 'cosine': 0.5917517095361371,\n",
       " 'tf_idf': 0.0239907400293694,\n",
       " 'bool_and': 0,\n",
       " 'bool_or': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor.get_feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 ms ± 656 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tmp_1= [Counter(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.81 s ± 14.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tmp_2= [text2lm(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'친구': 1,\n",
       "  '꺼': 1,\n",
       "  '빌려서': 1,\n",
       "  '써': 1,\n",
       "  '봤': 1,\n",
       "  '는데': 2,\n",
       "  '정말': 1,\n",
       "  '짱짱': 1,\n",
       "  '하': 2,\n",
       "  '게': 1,\n",
       "  '잘': 2,\n",
       "  '올라가': 1,\n",
       "  '힘': 1,\n",
       "  '조절': 1,\n",
       "  '못': 1,\n",
       "  '시': 1,\n",
       "  '면': 1,\n",
       "  '속눈썹': 1,\n",
       "  '이': 2,\n",
       "  '쪼금': 1,\n",
       "  '빠진다는': 1,\n",
       "  '점': 1,\n",
       "  '아쉬워': 1,\n",
       "  '요': 1},\n",
       " 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2lm(docs[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(map(len, docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕', '디지몬']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mecab.morphs('안녕 디지몬')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사전 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mecab import update_custom_dictionary\n",
    "# update_custom_dictionary('/root/custom_dict/ap_custom_dict.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 돌다.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('노란', 'VA+ETM'), ('기', 'ETN')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = \"노란기\"\n",
    "tokenizer.mecab.pos(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(DocId='20210308-103506-912355', offset=0, start=0, end=3, text='노란기')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(doc).Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 리무버\n",
    "    - 아이리무버\n",
    "    - 네일리무버\n",
    "    - ??\n",
    "- 브러쉬\n",
    "    \n",
    "    - 쉐도우\n",
    "    - 메컵\n",
    "    - 메컵베이스\n",
    "- 원플러스원\n",
    "    - 원플\n",
    "    - 원플원\n",
    "    - 원플러스원\n",
    "- 리뉴얼\n",
    "- 오일리\n",
    "- 왁싱\n",
    "- 가성비\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_docs=[]\n",
    "for i, doc in enumerate(docs):\n",
    "    proc_docs.append(tokenizer.mecab.morphs(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizable=False 제거\n",
    "- pos 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand = {'헤라' : ['헤라'], '랑콤' : ['랑콤'], '베네피트'  : ['베네피트']}\n",
    "\n",
    "class SBSAnalyzer:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    def _ngram(self):\n",
    "        return\n",
    "    def token2count(self):\n",
    "        return\n",
    "    def document_term_matrix(self):\n",
    "        return\n",
    "    def network(self):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def prevalence(self):\n",
    "        return\n",
    "    def diversity(self):\n",
    "        return\n",
    "    \n",
    "    def connectivity(self):\n",
    "        return\n",
    "    \n",
    "    def sbs(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상위 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### token2count\n",
    "- list of tokens을 list of counter로 만들기.\n",
    "- stem, lemantization 구현\n",
    "- (추가) 적절한 pos와 조합을 고민하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2count(tokens):\n",
    "    \"\"\"\n",
    "    list of Tokens -> counts\n",
    "    \"\"\"\n",
    "    pos = {'KEYPHRASE', 'NNG', 'NNP', 'VV', 'VA', 'XR', 'SL'} # 이건 Init으로 빼자.\n",
    "    keywords = []\n",
    "    for tok in tokens:\n",
    "        if set(tok._pos.split('+')).intersection(pos):\n",
    "            # stemming\n",
    "            if '+' in tok._pos:\n",
    "                for s in tok.expression.split('+'):\n",
    "                    a, b, _= s.split('/')\n",
    "                    if b in pos:\n",
    "                        stem = a\n",
    "                        p = b\n",
    "            else:\n",
    "                stem = tok.text\n",
    "                p = tok._pos\n",
    "\n",
    "            # lemmantization\n",
    "            if p in {'VV', 'VA'}:\n",
    "                stem = f\"{stem}다\"\n",
    "            keywords.append(stem)        \n",
    "    return Counter(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def _ngram(self, unigram: List[Token], n: int) -> List[Token]:\n",
    "#         return [ngram for ngram in zip(*[unigram[i:] for i in range(n)])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['약산성',\n",
       " '이',\n",
       " '라',\n",
       " '순',\n",
       " '하',\n",
       " '긴',\n",
       " '한데',\n",
       " '세정력',\n",
       " '이',\n",
       " '엄청',\n",
       " '좋',\n",
       " '은',\n",
       " '건',\n",
       " '아니',\n",
       " '에요',\n",
       " 'ㅎㅎ',\n",
       " '향',\n",
       " '은',\n",
       " '무',\n",
       " '향',\n",
       " '이',\n",
       " '고',\n",
       " '닦',\n",
       " '고',\n",
       " '폼',\n",
       " '클렌징',\n",
       " '으로',\n",
       " '2',\n",
       " '차',\n",
       " '세안',\n",
       " '필수',\n",
       " '입',\n",
       " '니당',\n",
       " '!',\n",
       " '클렌징',\n",
       " '워터',\n",
       " '만',\n",
       " '쓰',\n",
       " '시',\n",
       " '는',\n",
       " '분',\n",
       " '들',\n",
       " '께',\n",
       " '는',\n",
       " '비추',\n",
       " '에',\n",
       " '요',\n",
       " '~',\n",
       " '그렇',\n",
       " '다고',\n",
       " '세정력',\n",
       " '이',\n",
       " '안',\n",
       " '좋',\n",
       " '은',\n",
       " '건',\n",
       " '아니',\n",
       " '고',\n",
       " '바이',\n",
       " '오더',\n",
       " '마',\n",
       " '보다',\n",
       " '는',\n",
       " '잔여물',\n",
       " '이',\n",
       " '계속',\n",
       " '남',\n",
       " '는',\n",
       " '느낌',\n",
       " '입니다',\n",
       " '!']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda doc:Counter(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dtm\n",
    "- dtm 행렬 만들기.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_matrix(count_docs):\n",
    "    \"\"\"\n",
    "    * candidate selection은 preprocesseddocumnets에 구현.\n",
    "    * candidate weighting\n",
    "    \"\"\"\n",
    "    # Define idx2token and token2idx for represent the data by matrix\n",
    "    idx2token = sorted(dict(ChainMap(*count_docs)).keys())\n",
    "    token2idx = {tok:i for i, tok in enumerate(idx2token)}\n",
    "    \n",
    "    # Transform list-of-dict to document-term-matrix using sparse matrix\n",
    "    rows = list(chain(*[[doc_idx] * len(doc) for doc_idx, doc in enumerate(count_docs)])) # for (i,j)~DTM row-wise index position\n",
    "    cols, data = list(zip(*chain(*[doc.items() for doc in count_docs]))) # term keywords, data is frequence\n",
    "    cols = [token2idx[c] for c in cols] # transform term keyword to for (i,j)~DTM column-wise index position\n",
    "    dtm = csr_matrix((data, (rows, cols)))\n",
    "    \n",
    "    return dtm, idx2token, token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm, idx2token, token2idx = document_term_matrix(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(norm=False)\n",
    "dtm = tfidf.fit_transform(dtm)\n",
    "scores = np.squeeze(np.asarray(dtm.sum(axis=0))) # document-wise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = int(len(scores) * 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrase_args = np.argsort(-scores)[:top_k]\n",
    "keyphrase = [(idx2token[i], scores[i]) for i in keyphrase_args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2505"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keyphrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['헤라', '랑콤']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brands = {'헤라', '랑콤'}\n",
    "[x for x, _ in keyphrase if x in brands]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBS\n",
    "- Prevalence\n",
    "\n",
    "We can now proceed with the calculation of prevalence, which counts the frequency of occurrence of each brand name —  subsequently standardized considering the scores of all the words in the texts. My choice of standardization here is to subtract the mean and divide by the standard deviation. Other approaches are also possible. This step is important to compare measures carried out considering different time frames or sets of documents (e.g. brand importance on Twitter in April and May). Normalization of absolute scores is necessary before summing prevalence, diversity and connectivity to obtain the Semantic Brand Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#PREVALENCE\n",
    "#Import Counter and Numpy\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "#Create a dictionary with frequency counts for each word\n",
    "countPR = Counter()\n",
    "for t in texts:\n",
    "    countPR.update(Counter(t))\n",
    "\n",
    "#Calculate average score and standard deviation\n",
    "avgPR = np.mean(list(countPR.values()))\n",
    "stdPR = np.std(list(countPR.values()))\n",
    "\n",
    "#Calculate standardized Prevalence for each brand\n",
    "PREVALENCE = {}\n",
    "for brand in brands:\n",
    "    PREVALENCE[brand] = (countPR[brand] - avgPR) / stdPR\n",
    "    print(\"Prevalence\", brand, PREVALENCE[brand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mean, std는 전체에서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.mean(scores)\n",
    "std = np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence_top_k = [(tok, (s-mu)/std) for tok, s in keyphrase]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- DIVERSITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_dtm = dtm[:,keyphrase_args]\n",
    "g_idx2token = [idx2token[idx] for idx in keyphrase_args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurence = g_dtm.T.dot(g_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(co_occurence.toarray(), index=g_idx2token, columns=g_idx2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list = pd.melt(df.reset_index(), id_vars=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_list.columns = ['source','target','weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6275025, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_filter = 2\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(edge_list.source.unique()) # node 추가\n",
    "edge_set = [(s,t,{'weight':w}) for s,t,w in edge_list.loc[edge_list.weight > link_filter].values] # weight == 0인 엣지는 제외."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edges_from(edge_set) # edge 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate\n",
    "isolates = set(nx.isolates(G))\n",
    "G.remove_nodes_from(isolates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Loops will be ignored.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'brands' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9ceb9eeb2979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Calculate standardized Diversity for each brand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mDIVERSITY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbrand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbrands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mDIVERSITY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrand\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDIVERSITY_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrand\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mavgDI\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstdDI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Diversity\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIVERSITY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrand\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'brands' is not defined"
     ]
    }
   ],
   "source": [
    "#INSTALL AND IMPORT THE DISTINCTIVENESS PACKAGE\n",
    "#pip install -U distinctiveness\n",
    "from distinctiveness.dc import distinctiveness\n",
    "\n",
    "#DIVERSITY\n",
    "#Calculate Distinctiveness Centrality\n",
    "DC = distinctiveness(G, normalize = False, alpha = 1)\n",
    "DIVERSITY_sequence=DC[\"D2\"]\n",
    "\n",
    "#Calculate average score and standard deviation\n",
    "avgDI = np.mean(list(DIVERSITY_sequence.values()))\n",
    "stdDI = np.std(list(DIVERSITY_sequence.values()))\n",
    "#Calculate standardized Diversity for each brand\n",
    "DIVERSITY = {}\n",
    "for brand in brands:\n",
    "    DIVERSITY[brand] = (DIVERSITY_sequence[brand] - avgDI) / stdDI\n",
    "    print(\"Diversity\", brand, DIVERSITY[brand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define inverse weights \n",
    "for u,v,data in G_filtered.edges(data=True):\n",
    "    if 'weight' in data and data['weight'] != 0:\n",
    "        data['inverse'] = 1/data['weight']\n",
    "    else:\n",
    "        data['inverse'] = 1   \n",
    "\n",
    "#CONNECTIVITY\n",
    "CONNECTIVITY_sequence=nx.betweenness_centrality(G_filtered, normalized=False, weight ='inverse')\n",
    "#Calculate average score and standard deviation\n",
    "avgCO = np.mean(list(CONNECTIVITY_sequence.values()))\n",
    "stdCO = np.std(list(CONNECTIVITY_sequence.values()))\n",
    "#Calculate standardized Prevalence for each brand\n",
    "CONNECTIVITY = {}\n",
    "for brand in brands:\n",
    "    CONNECTIVITY[brand] = (CONNECTIVITY_sequence[brand] - avgCO) / stdCO\n",
    "    print(\"Connectivity\", brand, CONNECTIVITY[brand])\n",
    "Connectivity alice 0.6363388120984864\n",
    "Connectivity rabbit -0.05570445522732375\n",
    "The Semantic Brand Score of each brand is finally obtained by summing the standardized values of prevalence, diversity and connectivity. Different approaches are also possible.\n",
    "\n",
    "In [7]:\n",
    "#Obtain the Semantic Brand Score of each brand\n",
    "SBS = {}\n",
    "for brand in brands:\n",
    "    SBS[brand] = PREVALENCE[brand] + DIVERSITY[brand] + CONNECTIVITY[brand]\n",
    "    print(\"SBS\", brand, SBS[brand])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('어느', 'MM'), ('정도', 'NNG'), ('좋', 'VA'), ('네요', 'EC')]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mecab.pos('어느정도 좋네요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'semantic': None,\n",
       " 'has_jongseong': False,\n",
       " 'reading': '부드러워',\n",
       " 'type': 'Inflect',\n",
       " 'start_pos': 'VA',\n",
       " 'end_pos': 'EC',\n",
       " 'expression': '부드럽/VA/*+어/EC/*',\n",
       " '_pos': 'VA+EC',\n",
       " '_space': False}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[-5].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'데일리': 1,\n",
       "         '템': 2,\n",
       "         '헤어팩': 1,\n",
       "         '향': 1,\n",
       "         '좋다': 1,\n",
       "         '사용': 4,\n",
       "         '머릿결': 1,\n",
       "         '부드럽다': 1,\n",
       "         '지성': 1,\n",
       "         '피다': 1})"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "keywords = []\n",
    "for tok in tokens:\n",
    "    if set(tok._pos.split('+')).intersection(pos):\n",
    "        # stemming\n",
    "        if '+' in tok._pos:\n",
    "            for s in tok.expression.split('+'):\n",
    "                a, b, _= s.split('/')\n",
    "                if b in pos:\n",
    "                    stem = a\n",
    "                    p = b\n",
    "        else:\n",
    "            stem = tok.text\n",
    "            p = tok._pos\n",
    "        \n",
    "        # lemmantization\n",
    "        if p in {'VV', 'VA'}:\n",
    "            stem = f\"{stem}다\"\n",
    "        keywords.append(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['데일리',\n",
       " '템',\n",
       " '헤어팩',\n",
       " '향',\n",
       " '좋다',\n",
       " '사용',\n",
       " '템',\n",
       " '사용',\n",
       " '머릿결',\n",
       " '부드럽다',\n",
       " '지성',\n",
       " '피다',\n",
       " '사용',\n",
       " '사용']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('지성', 'NNG'), ('두', 'JX'), ('피', 'VV'), ('라', 'EC')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.mecab.pos('지성두피라')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('데일리', 'NNP'),\n",
       " ('템', 'NNG'),\n",
       " ('이', 'JKS'),\n",
       " ('헤어팩', 'KEYPHRASE'),\n",
       " ('향', 'NNG'),\n",
       " ('이', 'JKS'),\n",
       " ('좋', 'VA'),\n",
       " ('아서', 'EC'),\n",
       " ('계속', 'MAG'),\n",
       " ('계속', 'MAG'),\n",
       " ('사용', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('는', 'ETM'),\n",
       " ('템', 'NNG'),\n",
       " ('입니다', 'VCP+EF'),\n",
       " ('.', 'SF'),\n",
       " ('이것', 'NP'),\n",
       " ('을', 'JKO'),\n",
       " ('사용', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('면', 'EC'),\n",
       " ('머릿결', 'NNG'),\n",
       " ('이', 'JKS'),\n",
       " ('부드러워', 'VA+EC'),\n",
       " ('지', 'VX'),\n",
       " ('긴', 'ETN+JX'),\n",
       " ('한데', 'VX+EF'),\n",
       " ('.', 'SF'),\n",
       " ('..', 'SY'),\n",
       " ('지성', 'NNG'),\n",
       " ('두', 'JX'),\n",
       " ('피', 'VV'),\n",
       " ('라', 'EF'),\n",
       " ('.', 'SF'),\n",
       " ('.', 'SY'),\n",
       " ('자주', 'MAG'),\n",
       " ('사용', 'NNG'),\n",
       " ('하', 'XSV'),\n",
       " ('기', 'ETN'),\n",
       " ('에', 'JKB'),\n",
       " ('는', 'JX'),\n",
       " ('.', 'SF'),\n",
       " ('.', 'SY'),\n",
       " ('그래서', 'MAJ'),\n",
       " ('가끔', 'MAG'),\n",
       " ('씩', 'XSN'),\n",
       " ('사용', 'NNG'),\n",
       " ('중', 'NNB'),\n",
       " ('입니다', 'VCP+EF'),\n",
       " ('!', 'SF')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(tok.text, tok._pos) for tok in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['데일리',\n",
       " '템',\n",
       " '헤어팩',\n",
       " '향',\n",
       " '좋',\n",
       " '사용',\n",
       " '템',\n",
       " '사용',\n",
       " '머릿결',\n",
       " '어',\n",
       " '지성',\n",
       " '피',\n",
       " '사용',\n",
       " '사용']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNP'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Token' object has no attribute 'pos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-ba65b3e17bb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Token' object has no attribute 'pos'"
     ]
    }
   ],
   "source": [
    "tokens[0].pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'VV', 'VA'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemantize(token):\n",
    "    if ''\n",
    "    token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VCP+EF'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[-2]._pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VV'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'VV', 'EC'}.intersection(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SF']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[-1]._pos.split('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EF'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[14]._pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_term_matrix(count_docs):\n",
    "    \"\"\"\n",
    "    * candidate selection은 preprocesseddocumnets에 구현.\n",
    "    * candidate weighting\n",
    "    \"\"\"\n",
    "    # Define idx2token and token2idx for represent the data by matrix\n",
    "    idx2token = sorted(dict(ChainMap(*count_docs)).keys())\n",
    "    token2idx = {tok:i for i, tok in enumerate(idx2token)}\n",
    "    \n",
    "    # Transform list-of-dict to document-term-matrix using sparse matrix\n",
    "    rows = list(chain(*[[doc_idx] * len(doc) for doc_idx, doc in enumerate(count_docs)])) # for (i,j)~DTM row-wise index position\n",
    "    cols, data = list(zip(*chain(*[doc.items() for doc in count_docs]))) # term keywords, data is frequence\n",
    "    cols = [token2idx[c] for c in cols] # transform term keyword to for (i,j)~DTM column-wise index position\n",
    "    dtm = csr_matrix((data, (rows, cols)))\n",
    "    \n",
    "    return dtm, idx2token, token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "335"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer._normalize(docs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer._preprocess(docs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'전 02 그레이 브라운 쓰고 있는데요 이거 1년이 지나도 쓰고 있는 아이템 떨어트리지만 않으면 괜찮은 아이템 ㅎㅎ 깨지기쉬운 딱 그런타입 하지만 눈썹 브러쉬도 맘에들고 양쪽 섞어 바르면 자연스러운 눈썹 연출하기 너무 좋아요 단 너무 찍어 바르면 눈썹만 동동 떠보인다는.. 색감조절 양조절 손등에 테스트 해보시고 발라보세요 !! 한번 떨어트렸다가 진심 쪼개져서 난감햇다는 .. 어휴 양 진짜 안줄어요 ㅋㅋㅋㅋㅋㅋ신기해요 앞으로 2년은 더쓸걸요 뭉치지 않게만 살살 브러쉬로 풀어주면 잘 지워지지도 않고 정말 유용템임!!  눈썹브로우추천   눈썹브로우추천   눈썹브로우추천   눈썹브로우추천'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._preprocess(docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "re.compile(r'#평가단\\*', re.UNICODE)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.compile('#평가단\\*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f'#([{emojis}\\w-]+)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['눈썹브로우추천', '자연스러운브로우', '오래쓰는브로우', '더페이스샵브로우마스터아이브로우키트추천']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.hashtag.findall(docs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'전 02 그레이 브라운 쓰고 있는데요 이거 1년이 지나도 쓰고 있는 아이템 떨어트리지만 않으면 괜찮은 아이템 ㅎㅎ 깨지기쉬운 딱 그런타입 하지만 눈썹 브러쉬도 맘에들고 양쪽 섞어 바르면 자연스러운 눈썹 연출하기 너무 좋아요 단 너무 찍어 바르면 눈썹만 동동 떠보인다는.. 색감조절 양조절 손등에 테스트 해보시고 발라보세요 !! 한번 떨어트렸다가 진심 쪼개져서 난감햇다는 .. 어휴 양 진짜 안줄어요 ㅋㅋㅋㅋㅋㅋ신기해요 앞으로 2년은 더쓸걸요 뭉치지 않게만 살살 브러쉬로 풀어주면 잘 지워지지도 않고 정말 유용템임!! #눈썹브로우추천 #자연스러운브로우 #오래쓰는브로우 #더페이스샵브로우마스터아이브로우키트추천'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.textanalyzer import KoPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = KoPreprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'느그'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"느그{\".strip(\"$()*+.?[\\^{|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile(f\"#{process.hashtag.findall(docs[445])[0].strip('*!')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "f-string expression part cannot include a backslash (<ipython-input-27-74d8cffb7542>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-74d8cffb7542>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    p.sub(f\" {process.hashtag.findall(docs[445])[0].strip('$()*+.?[\\^{|')} \", docs[445])\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m f-string expression part cannot include a backslash\n"
     ]
    }
   ],
   "source": [
    "p.sub(f\" {process.hashtag.findall(docs[445])[0].strip('*!')} \", docs[445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 평가단리뷰  피부타입 :티존 지성, 유존 건성인 복합성피부 (가끔 여드름 올라옴)* 향 :오렌지껍질향 (전성분 맨마지막 왕귤껍질오일 때문인거같네요.* 제형 :묽은 로션+수분크림 느낌* 느낌 :수분감이 넘쳤고 그렇다고 수분감만 넘치면 건조한느낌이 빨리 오는데 유분도 적당하게 들어가서 건조해지지 않았어요.수분감도 팍! 유분감도 팍!피부가 예민해서 트러블 잘 올라오는데 다행히 이제품은 트러블 없습니다.수분크림없어도 이 제품만 발라도 되더라구요.저에게 가장 큰 효과는 속건조가 느껴지지않아서 요즘도 잘 쓰고있습니다.* 아쉬운점:용기가 바디로션용처럼 나온것같아요. 한번 펌핑하면 양이 너무 많아서 반만 펌핑중입니다. 가격을 생각하면 용기도 좀 저렴해보이구여용량이 적어 아쉽네요.제품을 무상으로 제공받아 사용 후 솔직하게 작성된 리뷰입니다.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.hashtag.sub(f\" {'평가단리뷰'} \",docs[445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs[445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(docs[445])[' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19464720194647203"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "80/411"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 평가단리뷰* * 피부타입 :티존 지성, 유존 건성인 복합성피부 (가끔 여드름 올라옴)* 향 :오렌지껍질향 (전성분 맨마지막 왕귤껍질오일 때문인거같네요.* 제형 :묽은 로션+수분크림 느낌* 느낌 :수분감이 넘쳤고 그렇다고 수분감만 넘치면 건조한느낌이 빨리 오는데 유분도 적당하게 들어가서 건조해지지 않았어요.수분감도 팍! 유분감도 팍!피부가 예민해서 트러블 잘 올라오는데 다행히 이제품은 트러블 없습니다.수분크림없어도 이 제품만 발라도 되더라구요.저에게 가장 큰 효과는 속건조가 느껴지지않아서 요즘도 잘 쓰고있습니다.* 아쉬운점:용기가 바디로션용처럼 나온것같아요. 한번 펌핑하면 양이 너무 많아서 반만 펌핑중입니다. 가격을 생각하면 용기도 좀 저렴해보이구여용량이 적어 아쉽네요.제품을 무상으로 제공받아 사용 후 솔직하게 작성된 리뷰입니다.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.replace_hashtag(docs[445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'평가단리뷰* * 피부타입 :티존 지성, 유존 건성인 복합성피부 (가끔 여드름 올라옴)* 향 :오렌지껍질향 (전성분 맨마지막 왕귤껍질오일 때문인거같네요.* 제형 :묽은 로션+수분크림 느낌* 느낌 :수분감이 넘쳤고 그렇다고 수분감만 넘치면 건조한느낌이 빨리 오는데 유분도 적당하게 들어가서 건조해지지 않았어요.수분감도 팍! 유분감도 팍!피부가 예민해서 트러블 잘 올라오는데 다행히 이제품은 트러블 없습니다.수분크림없어도 이 제품만 발라도 되더라구요.저에게 가장 큰 효과는 속건조가 느껴지지않아서 요즘도 잘 쓰고있습니다.* 아쉬운점:용기가 바디로션용처럼 나온것같아요. 한번 펌핑하면 양이 너무 많아서 반만 펌핑중입니다. 가격을 생각하면 용기도 좀 저렴해보이구여용량이 적어 아쉽네요.제품을 무상으로 제공받아 사용 후 솔직하게 작성된 리뷰입니다.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._preprocess(docs[445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#평가단리뷰* 피부타입 :티존 지성, 유존 건성인 복합성피부 (가끔 여드름 올라옴)* 향 :오렌지껍질향 (전성분 맨마지막 왕귤껍질오일 때문인거같네요.* 제형 :묽은 로션+수분크림 느낌* 느낌 :수분감이 넘쳤고 그렇다고 수분감만 넘치면 건조한느낌이 빨리 오는데 유분도 적당하게 들어가서 건조해지지 않았어요.수분감도 팍! 유분감도 팍!피부가 예민해서 트러블 잘 올라오는데 다행히 이제품은 트러블 없습니다.수분크림없어도 이 제품만 발라도 되더라구요.저에게 가장 큰 효과는 속건조가 느껴지지않아서 요즘도 잘 쓰고있습니다.* 아쉬운점:용기가 바디로션용처럼 나온것같아요. 한번 펌핑하면 양이 너무 많아서 반만 펌핑중입니다. 가격을 생각하면 용기도 좀 저렴해보이구여용량이 적어 아쉽네요.제품을 무상으로 제공받아 사용 후 솔직하게 작성된 리뷰입니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._normalize(docs[445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pororo import Pororo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-03 22:48:42,992-INFO: [input] dictionary: 4005 types\n",
      "2021-03-03 22:48:42,994-INFO: [label] dictionary: 9 types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of now, this beta model tries to correct spacing errors in Korean text.\n"
     ]
    }
   ],
   "source": [
    "spacing= Pororo(task='gec', lang='ko')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#평가단리뷰* 피부타입 :티존 지성, 유존 건성인 복합성피부 (가끔 여드름 올라옴)* 향 :오렌지껍질향 (전성분 맨마지막 왕귤껍질오일 때문인거같네요.* 제형 :묽은 로션+수분크림 느낌* 느낌 :수분감이 넘쳤고 그렇다고 수분감만 넘치면 건조한느낌이 빨리 오는데 유분도 적당하게 들어가서 건조해지지 않았어요.수분감도 팍! 유분감도 팍!피부가 예민해서 트러블 잘 올라오는데 다행히 이제품은 트러블 없습니다.수분크림없어도 이 제품만 발라도 되더라구요.저에게 가장 큰 효과는 속건조가 느껴지지않아서 요즘도 잘 쓰고있습니다.* 아쉬운점:용기가 바디로션용처럼 나온것같아요. 한번 펌핑하면 양이 너무 많아서 반만 펌핑중입니다. 가격을 생각하면 용기도 좀 저렴해보이구여용량이 적어 아쉽네요.제품을 무상으로 제공받아 사용 후 솔직하게 작성된 리뷰입니다.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[445]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MecabTokenization(custom_dir='/root/custom_dict')\n",
    "\n",
    "pos = {'KEYPHRASE', 'NNG', 'NNP', 'VV', 'VA', 'XR', 'SL', 'SY'}\n",
    "\n",
    "doc = '손에 붙거나 머리에 발랐을때 뭉치는 현상 없어서 좋아요. 지금까지 삿던 오일제품중 제일 좋아요. 계속 써보고 다음에도 구입할께요'\n",
    "[t for t, p in tokenizer(doc) if p in pos]\n",
    "\n",
    "\n",
    "\n",
    "19 + 2 + 738 + 6 + 63 + 712 + 879 + 1946\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "chain()\n",
    "\n",
    "\n",
    "\n",
    "tokenizer(doc).Tokens[3]\n",
    "\n",
    "\n",
    "\n",
    "class SBSAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.pos = {'KEYPHRASE', 'NNG', 'NNP', 'VV', 'VA', 'XR', 'SL', 'SY'}\n",
    "        \n",
    "    @property\n",
    "    def keyphrase(self):\n",
    "        \n",
    "\n",
    "class MecabTCG(TokenCandidateGeneration):\n",
    "    def __init__(self, ngram=1):\n",
    "        self.pos = {'KEYPHRASE', 'NNG', 'NNP', 'VV', 'VA', 'XR', 'SL', 'SY'}\n",
    "        self.N = ngram\n",
    "    \n",
    "    def get_candidate(self, doc: Doc) -> Doc:\n",
    "        if doc.tokenizable:\n",
    "            unigram = [token for token in doc.tokens if (token.pos in self.pos) and (token.text.strip() != '')]\n",
    "            candidates = [unigram]\n",
    "            if self.N > 1:\n",
    "                for i in range(1, self.N):\n",
    "                    candidates.append(self._ngram(unigram, (i+1)))\n",
    "            doc.candidates = candidates\n",
    "            return doc\n",
    "        else:\n",
    "            doc.candidates = [[]]\n",
    "            return doc\n",
    "    \n",
    "    def _ngram(self, unigram: List[Token], n: int) -> List[Token]:\n",
    "        return [ngram for ngram in zip(*[unigram[i:] for i in range(n)])]\n",
    "\n",
    "- keywords\n",
    "    - 다양한 로직으로 정해짐.\n",
    "- prevalence\n",
    "- diversity\n",
    "- connectivity\n",
    "- sbs\n",
    "\n",
    "- brand how?\n",
    "\n",
    "prevalence\n",
    "diversity\n",
    "\n",
    "다큐먼트...\n",
    "\n",
    "\n",
    "tokens = tokenizer('커버력짱인듯 다크닝도없고 참 좋아요 프라이머랑 같이구매했는데 아직까진 대만족합니당수정화장용으로 좋은거같아요~').Tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
